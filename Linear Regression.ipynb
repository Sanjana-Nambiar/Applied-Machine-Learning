{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61457c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nyuad/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085b230",
   "metadata": {},
   "source": [
    "# 1 & 2. Exploration of the dataset & predicting price of a house\n",
    "a. Explore the dataset by using the Scikit Learn library and Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "376ea049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid printing out warnings\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20047c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((506, 13), (506,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d8c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.320e-03, 1.800e+01, 2.310e+00, 0.000e+00, 5.380e-01, 6.575e+00,\n",
       "        6.520e+01, 4.090e+00, 1.000e+00, 2.960e+02, 1.530e+01, 3.969e+02,\n",
       "        4.980e+00]),\n",
       " 24.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0] , y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d67b9a",
   "metadata": {},
   "source": [
    "# 3. Linear Regression closed form with kfold validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edd3fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average mean square error on the training set: 21.8021551962688\n",
      "The average mean square error on the test set: 23.735710217214514\n"
     ]
    }
   ],
   "source": [
    "# Append a column of ones to the feature matrix to accommodate the bias\n",
    "X_b = np.c_[np.ones((X.shape[0], 1)), X] \n",
    "y = np.array([y]).T  \n",
    "\n",
    "# mean squared errors for training and test sets\n",
    "mse_train = 0\n",
    "mse_test = 0\n",
    "\n",
    "# 10 splits \n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train_b, X_test_b = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Fit the model using the closed-form solution\n",
    "    theta = np.linalg.inv(X_train_b.T.dot(X_train_b)).dot(X_train_b.T).dot(y_train)\n",
    "    \n",
    "    # Calculate mean squared error on training set & test set \n",
    "    mse_train += np.mean((y_train - X_train_b.dot(theta))**2)\n",
    "    mse_test += np.mean((y_test - X_test_b.dot(theta))**2)\n",
    "\n",
    "\n",
    "print(f\"The average mean square error on the training set: {mse_train/10}\")\n",
    "print(f\"The average mean square error on the test set: {mse_test/10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3113720",
   "metadata": {},
   "source": [
    "# 4. & 5. Ridge Regression with KFold Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5330782d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model with the least test error is with alpha value 10.0\n",
      "\n",
      "The training error of the best model is 22.61241006057968\n",
      "The testing error of the best model is 24.063665406398414\n"
     ]
    }
   ],
   "source": [
    "A = np.eye(X_b.shape[1])\n",
    "A[0][0] = 0\n",
    "\n",
    "alpha = np.logspace(1, 7, num=13)\n",
    "\n",
    "mse_train_alphas = []\n",
    "mse_test_alphas = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "for a in alpha:\n",
    "    mse_train = 0\n",
    "    mse_test = 0\n",
    "    for train_index, test_index in kf.split(X_b):\n",
    "        # Split the data into training and test sets\n",
    "        X_train_b, X_test_b = X_b[train_index], X_b[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit the model using the closed-form solution\n",
    "        theta = np.linalg.inv(X_train_b.T.dot(X_train_b) + a * A).dot(X_train_b.T).dot(y_train)\n",
    "\n",
    "        # Calculate mean squared error on training set & test set\n",
    "        mse_train += np.mean((y_train - X_train_b.dot(theta))**2)\n",
    "        mse_test += np.mean((y_test - X_test_b.dot(theta))**2)\n",
    "\n",
    "    #keeing track of the mse errors \n",
    "    mse_train_alphas.append(mse_train / 10) \n",
    "    mse_test_alphas.append(mse_test / 10)\n",
    "\n",
    "#finding the best alpha based on the error\n",
    "index = np.argmin(mse_test_alphas)\n",
    "\n",
    "print(f\"The best model with the least test error is with alpha value {alpha[index]}\\n\") \n",
    "print(f\"The training error of the best model is {mse_train_alphas[index]}\")\n",
    "print(f\"The testing error of the best model is {mse_test_alphas[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0ce825",
   "metadata": {},
   "source": [
    "# 6. Polynomial Transformation & ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83fbb6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model with the least test error is with alpha value 316.22776601683796\n",
      "\n",
      "The training error of the best model is 8.110304254637338\n",
      "The testing error of the best model is 13.453423742307717\n"
     ]
    }
   ],
   "source": [
    "# perform a polynomial features transform of the dataset\n",
    "trans = PolynomialFeatures(degree=2)\n",
    "\n",
    "# applying polynomial transformation only on the X and not X with bias because the function already adds it\n",
    "X_b_transformed = trans.fit_transform(X) \n",
    "\n",
    "A = np.eye(X_b_transformed.shape[1])\n",
    "A[0][0] = 0\n",
    "\n",
    "alpha = np.logspace(1, 7, num=13)\n",
    "\n",
    "mse_train_alphas = []\n",
    "mse_test_alphas = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "for a in alpha:\n",
    "    mse_train = 0\n",
    "    mse_test = 0\n",
    "    for train_index, test_index in kf.split(X_b_transformed):\n",
    "        \n",
    "        # Split the data into training and test sets\n",
    "        X_train_b, X_test_b = X_b_transformed[train_index], X_b_transformed[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # Fit the model using the closed-form solution \n",
    "        theta = np.linalg.inv(X_train_b.T.dot(X_train_b) + a * A).dot(X_train_b.T).dot(y_train)\n",
    "\n",
    "        # Calculate mean squared error on training set & test set\n",
    "        mse_train += np.mean((y_train - X_train_b.dot(theta))**2)\n",
    "        mse_test += np.mean((y_test - X_test_b.dot(theta))**2)\n",
    "\n",
    "    mse_train_alphas.append(mse_train / 10)\n",
    "    mse_test_alphas.append(mse_test / 10)\n",
    "\n",
    "index = np.argmin(mse_test_alphas)\n",
    "\n",
    "print(f\"The best model with the least test error is with alpha value {alpha[index]}\\n\") \n",
    "print(f\"The training error of the best model is {mse_train_alphas[index]}\")\n",
    "print(f\"The testing error of the best model is {mse_test_alphas[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cf009",
   "metadata": {},
   "source": [
    "# 7. Gradient Descent method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6570b816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 1 ...\n",
      "Kfold split 1: Training Loss: 25.731080395790936, Test Loss: 18.80952909763336\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 2 ...\n",
      "Kfold split 2: Training Loss: 25.478865993486803, Test Loss: 18.809288111996835\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 3 ...\n",
      "Kfold split 3: Training Loss: 24.916301067256153, Test Loss: 24.046520126562616\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 4 ...\n",
      "Kfold split 4: Training Loss: 25.27687550367338, Test Loss: 22.835023399552906\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 5 ...\n",
      "Kfold split 5: Training Loss: 24.92925333371076, Test Loss: 25.35940772017966\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 6 ...\n",
      "Kfold split 6: Training Loss: 25.16636118886832, Test Loss: 23.423809414916644\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 7 ...\n",
      "Kfold split 7: Training Loss: 25.017185341520666, Test Loss: 25.59862016775408\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 8 ...\n",
      "Kfold split 8: Training Loss: 25.064304774235282, Test Loss: 25.077856781561994\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 9 ...\n",
      "Kfold split 9: Training Loss: 24.736853837935765, Test Loss: 27.335553484047423\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training 10 ...\n",
      "Kfold split 10: Training Loss: 24.88436728378569, Test Loss: 26.18840109779337\n",
      "---------------------------------------------------------------------------------->\n",
      "\n",
      "The average mean square error on the training set: 24.88436728378569\n",
      "The average mean square error on the test set: 26.18840109779337\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000035 # learning rate \n",
    "epochs = 500000\n",
    "n = 506 # no of data points\n",
    "\n",
    "# mean squared errors for training and test sets\n",
    "mse_train = 0\n",
    "mse_test = 0\n",
    "\n",
    "# 10 splits \n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "ctr = 1\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train_b, X_test_b = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # gradient descent \n",
    "    theta = np.random.randn(14,1) # random initialization\n",
    "    print(\"---------------------------------------------------------------------------------->\")\n",
    "    print(f\"Gradient descent training {ctr} ...\")\n",
    "   \n",
    "    for eps in range(epochs):\n",
    "        gradient = (2/n) * (X_train_b.T.dot(X_train_b.dot(theta) - y_train))\n",
    "        theta = theta - (lr * gradient) #training the theta\n",
    "   \n",
    "    # Calculate mean squared error on training set & test set \n",
    "    mse_train += np.mean((y_train - X_train_b.dot(theta))**2)\n",
    "    mse_test += np.mean((y_test - X_test_b.dot(theta))**2)\n",
    "    \n",
    "    print(f\"Kfold split {ctr}: Training Loss: {mse_train/ctr}, Test Loss: {mse_test/ctr}\")\n",
    "    ctr += 1\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------->\\n\")\n",
    "print(f\"The average mean square error on the training set: {mse_train/10}\")\n",
    "print(f\"The average mean square error on the test set: {mse_test/10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55583e",
   "metadata": {},
   "source": [
    "# 8. Lasso Regression \n",
    "a. Cost function of Lasso Regression:\n",
    "    ð½(ðœƒ) = ð‘€ð‘†ð¸(ðœƒ) + ð›¼âˆ‘|ðœƒð‘–|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c29b0905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 1 ...\n",
      "Kfold split 1: Training Loss: 26.51371939987463, Test Loss: 31.778551018684812\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 2 ...\n",
      "Kfold split 2: Training Loss: 26.316368715940243, Test Loss: 27.258857455000317\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 3 ...\n",
      "Kfold split 3: Training Loss: 26.375664001803898, Test Loss: 25.91459289265441\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 4 ...\n",
      "Kfold split 4: Training Loss: 26.571940963288696, Test Loss: 24.318012784769866\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 5 ...\n",
      "Kfold split 5: Training Loss: 26.88275504633267, Test Loss: 23.30440597287309\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 6 ...\n",
      "Kfold split 6: Training Loss: 26.735758416618804, Test Loss: 26.39912669760145\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 7 ...\n",
      "Kfold split 7: Training Loss: 26.560415708706408, Test Loss: 28.703869368128576\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 8 ...\n",
      "Kfold split 8: Training Loss: 26.496559257063083, Test Loss: 29.095083254423187\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 9 ...\n",
      "Kfold split 9: Training Loss: 26.438909494880242, Test Loss: 28.949712499782233\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of lasso regression 10 ...\n",
      "Kfold split 10: Training Loss: 26.523702657787773, Test Loss: 28.296396899978923\n",
      "---------------------------------------------------------------------------------->\n",
      "\n",
      "The average mean square error on the training set: 26.523702657787773\n",
      "The average mean square error on the test set: 28.296396899978923\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000035 # learning rate \n",
    "epochs = 500000\n",
    "n = 506 # no of data points\n",
    "\n",
    "# mean squared errors for training and test sets\n",
    "mse_train = 0\n",
    "mse_test = 0\n",
    "\n",
    "# 10 splits \n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "ctr = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X):    \n",
    "    # Split the data into training and test sets\n",
    "    X_train_b, X_test_b = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # gradient descent \n",
    "    theta = np.random.randn(14,1) # random initialization\n",
    "    print(\"---------------------------------------------------------------------------------->\")\n",
    "    print(f\"Gradient descent training with cost function of lasso regression {ctr} ...\")\n",
    "   \n",
    "    for eps in range(epochs): \n",
    "        # updating the gradient for lasso \n",
    "        gradient = ((2/n) * (X_train_b.T.dot(X_train_b.dot(theta) - y_train))) + np.sign(theta)\n",
    "        theta = theta - (lr * gradient)# training the theta\n",
    "   \n",
    "    # Calculate mean squared error on training set & test set \n",
    "    mse_train += np.mean((y_train - X_train_b.dot(theta))**2)\n",
    "    mse_test += np.mean((y_test - X_test_b.dot(theta))**2)\n",
    "    print(f\"Kfold split {ctr}: Training Loss: {mse_train/ctr}, Test Loss: {mse_test/ctr}\")\n",
    "    ctr += 1\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------->\\n\")\n",
    "print(f\"The average mean square error on the training set: {mse_train/10}\")\n",
    "print(f\"The average mean square error on the test set: {mse_test/10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5d9e34",
   "metadata": {},
   "source": [
    "# 9. Elastic Net \n",
    "a. Cost Function of Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b33a95b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 1 ...\n",
      "Kfold split 1: Training Loss: 30.78236934148858, Test Loss: 30.55225729675368\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 2 ...\n",
      "Kfold split 2: Training Loss: 30.670918839496032, Test Loss: 32.75151287766248\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 3 ...\n",
      "Kfold split 3: Training Loss: 30.96818131206629, Test Loss: 32.543029542639424\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 4 ...\n",
      "Kfold split 4: Training Loss: 30.77448095431799, Test Loss: 34.219945734556696\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 5 ...\n",
      "Kfold split 5: Training Loss: 30.511769355264384, Test Loss: 36.59531660348984\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 6 ...\n",
      "Kfold split 6: Training Loss: 30.712970421961234, Test Loss: 35.596345226402065\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 7 ...\n",
      "Kfold split 7: Training Loss: 30.798139070094628, Test Loss: 35.96275771603397\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 8 ...\n",
      "Kfold split 8: Training Loss: 30.937644624859193, Test Loss: 34.61419376226985\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 9 ...\n",
      "Kfold split 9: Training Loss: 31.062861959005634, Test Loss: 33.46598127319649\n",
      "---------------------------------------------------------------------------------->\n",
      "Gradient descent training with cost function of elastic regression 10 ...\n",
      "Kfold split 10: Training Loss: 31.14505168618797, Test Loss: 32.96488099952868\n",
      "---------------------------------------------------------------------------------->\n",
      "\n",
      "The average mean square error on the training set: 31.14505168618797\n",
      "The average mean square error on the test set: 32.96488099952868\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000035 # learning rate \n",
    "epochs = 500000\n",
    "n = 506 # no of data points\n",
    "a1 = 0.4\n",
    "\n",
    "# mean squared errors for training and test sets\n",
    "mse_train = 0\n",
    "mse_test = 0\n",
    "\n",
    "# 10 splits \n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "ctr = 1\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and test sets\n",
    "    X_train_b, X_test_b = X_b[train_index], X_b[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # gradient descent \n",
    "    theta = np.random.randn(14,1) # random initialization\n",
    "    print(\"---------------------------------------------------------------------------------->\")\n",
    "    print(f\"Gradient descent training with cost function of elastic regression {ctr} ...\")\n",
    "   \n",
    "    for eps in range(epochs):\n",
    "        gradient = ( \n",
    "            ((2/n) * (X_train_b.T.dot(X_train_b.dot(theta) - y_train))) #gradient of mse\n",
    "            + (2 * (1-a1) * theta) #gradient of l2 - penality \n",
    "            + (a1 * np.sign(theta))) #gradient of l1 - penality \n",
    "        theta = theta - (lr * gradient)# training the theta\n",
    "   \n",
    "    # Calculate mean squared error on training set & test set \n",
    "    mse_train += np.mean((y_train - X_train_b.dot(theta))**2)\n",
    "    mse_test += np.mean((y_test - X_test_b.dot(theta))**2)\n",
    "    print(f\"Kfold split {ctr}: Training Loss: {mse_train/ctr}, Test Loss: {mse_test/ctr}\")\n",
    "    ctr += 1\n",
    "\n",
    "print(\"---------------------------------------------------------------------------------->\\n\")\n",
    "print(f\"The average mean square error on the training set: {mse_train/10}\")\n",
    "print(f\"The average mean square error on the test set: {mse_test/10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4207e3b5",
   "metadata": {},
   "source": [
    "# 10. If you are given a choice of predicting future housing prices using one of the models you have learned above (those optimized with gradient descent), which one would you choose and why? State the parameters of that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f722cc",
   "metadata": {},
   "source": [
    "### From the above training that has been done above, I would use gradient descent to train this dataset as the test loss or the mse for this much less then that of others. The parameters of the model is the wieght and the bias. The learning rate and the number of epochs. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
